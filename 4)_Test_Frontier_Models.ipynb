{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Frontier Model Evaluation for Electronics Price Prediction\n",
        "\n",
        "## üìä Comparing the Accuracy of Cutting-Edge Language Models\n",
        "\n",
        "In this notebook, we evaluate the performance of advanced Frontier models on the task of predicting electronics prices based on structured prompt inputs.  \n",
        "The models tested include:\n",
        "\n",
        "- üîπ GPT-4o-mini  \n",
        "- üîπ GPT-4o  \n",
        "- üîπ Claude 3.7 Sonnet  \n",
        "- üîπ Google Gemini 1.5 Flash  \n",
        "- üîπ Google Gemini 2.0 Flash  \n",
        "- üîπ DeepSeek V3\n",
        "\n",
        "Each model receives a consistent system prompt and is tested on the same input format to ensure fair comparison.  \n",
        "Performance is assessed using metrics such as absolute error, RMSLE, and hit rate.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### ‚öôÔ∏è Runtime Info\n",
        "\n",
        "This notebook runs on a **simple CPU runtime**, as it only queries external APIs and doesn't require GPU acceleration.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è Important Notes\n",
        "\n",
        "It's important to appreciate that **we are not training the Frontier models**.  \n",
        "We're only providing them with the **test dataset** to observe how well they perform.  \n",
        "They do **not** receive the 400,000 training examples we used for the traditional ML models.\n",
        "\n",
        "**That said...**\n",
        "\n",
        "Given the massive scale of their training data, it's entirely possible these models have already seen some (or even all) of the products from both the training and test sets.  \n",
        "This could lead to **test contamination**, giving them an **unfair advantage**.  \n",
        "We should keep this possibility in mind when interpreting the results.\n",
        "\n",
        "This notebook runs on a simple cpu\n"
      ],
      "metadata": {
        "id": "YsmHfBaqp-my"
      },
      "id": "YsmHfBaqp-my"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets anthropic"
      ],
      "metadata": {
        "id": "jXClM45GUxMj"
      },
      "id": "jXClM45GUxMj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "681c717b-4c24-4ac3-a5f3-3c5881d6e70a",
      "metadata": {
        "id": "681c717b-4c24-4ac3-a5f3-3c5881d6e70a"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "from huggingface_hub import login\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from openai import OpenAI\n",
        "from anthropic import Anthropic\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset, Dataset, DatasetDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants - used for printing to stdout in color\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\"\n",
        "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}\n",
        "\n",
        "# Dataset\n",
        "HF_USER = \"vassilis19\" # your HF name here! Or use mine if you just want to reproduce my results.\n",
        "DATASET_NAME = f\"{HF_USER}/pricer-electronics-data\"\n",
        "REVISION = \"701eba81570388cfd60924c6fe144b27491a9ec0\""
      ],
      "metadata": {
        "id": "Ou1pSHV7VRRX"
      },
      "id": "Ou1pSHV7VRRX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36d05bdc-0155-4c72-a7ee-aa4e614ffd3c",
      "metadata": {
        "id": "36d05bdc-0155-4c72-a7ee-aa4e614ffd3c"
      },
      "outputs": [],
      "source": [
        "# üîê Load secret API keys from Colab's environment (must be added via the üîë \"Secrets\" panel)\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "deepseek_api_key = userdata.get('DEEPSEEK_API_KEY')\n",
        "\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîó Initialize OpenAI-compatible clients for third-party models\n",
        "openai = OpenAI(api_key= OPENAI_API_KEY)\n",
        "claude = Anthropic(api_key = ANTHROPIC_API_KEY)\n",
        "gemini_via_openai_client = OpenAI(                                   # Google Gemini (via OpenAI-compatible client)\n",
        "    api_key=google_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "deepseek_via_openai_client = OpenAI(                                 # DeepSeek (via OpenAI-compatible client)\n",
        "    api_key=deepseek_api_key,\n",
        "    base_url=\"https://api.deepseek.com\"\n",
        ")"
      ],
      "metadata": {
        "id": "CI5SO3uZJLxk"
      },
      "id": "CI5SO3uZJLxk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üì• Load the dataset from the Hugging Face Hub (using a specific revision for reproducibility)\n",
        "dataset = load_dataset(DATASET_NAME, revision = REVISION)\n",
        "train = dataset['train']\n",
        "test = dataset['test']"
      ],
      "metadata": {
        "id": "7tua83udUFwb"
      },
      "id": "7tua83udUFwb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä Tester class for evaluating price prediction models. Accepts a prediction function and a dataset, runs evaluation over a subset of items, calculates error metrics (absolute error, RMSLE), and visualizes predictions vs actual prices:\n"
      ],
      "metadata": {
        "id": "0bsxVLQGo93s"
      },
      "id": "0bsxVLQGo93s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6985bdc7-fa45-49a3-ae97-84bdeb9b2083",
      "metadata": {
        "id": "6985bdc7-fa45-49a3-ae97-84bdeb9b2083"
      },
      "outputs": [],
      "source": [
        "class Tester:\n",
        "    # Initialize the tester with a predictor function, dataset, optional title, and sample size\n",
        "    def __init__(self, predictor, data, title=None, size=250):\n",
        "        self.predictor = predictor\n",
        "        self.data = data\n",
        "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
        "        self.size = size  # Number of datapoints to test\n",
        "        self.guesses = []  # Model predictions\n",
        "        self.truths = []   # Ground truth prices\n",
        "        self.errors = []   # Absolute errors\n",
        "        self.sles = []     # Squared log errors\n",
        "        self.colors = []   # Color codes for visualization\n",
        "\n",
        "    # Determine color based on error severity for visualization\n",
        "    def color_for(self, error, truth):\n",
        "        if error < 40 or error / truth < 0.2:\n",
        "            return \"green\"\n",
        "        elif error < 80 or error / truth < 0.4:\n",
        "            return \"orange\"\n",
        "        else:\n",
        "            return \"red\"\n",
        "\n",
        "    # Run prediction and error calculation for a single datapoint\n",
        "    def run_datapoint(self, i):\n",
        "        datapoint = self.data[i]\n",
        "        guess = self.predictor(datapoint[\"text\"])  # Run the model\n",
        "        truth = datapoint[\"price\"]  # True price\n",
        "        error = abs(guess - truth)  # Absolute error\n",
        "        log_error = math.log(truth + 1) - math.log(guess + 1)  # Log error\n",
        "        sle = log_error ** 2  # Squared log error\n",
        "        color = self.color_for(error, truth)  # Color for this point\n",
        "        title = datapoint[\"text\"].split(\"\\n\\n\")[1][:20] + \"...\"  # Short title snippet for display\n",
        "        # Record values for reporting\n",
        "        self.guesses.append(guess)\n",
        "        self.truths.append(truth)\n",
        "        self.errors.append(error)\n",
        "        self.sles.append(sle)\n",
        "        self.colors.append(color)\n",
        "        # Print detailed result for the datapoint\n",
        "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
        "\n",
        "    # Create a scatter plot of predictions vs ground truth\n",
        "    def chart(self, title):\n",
        "        max_error = max(self.errors)\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        max_val = max(max(self.truths), max(self.guesses))\n",
        "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)  # Diagonal line\n",
        "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)  # Plot points\n",
        "        plt.xlabel('Ground Truth')\n",
        "        plt.ylabel('Model Estimate')\n",
        "        plt.xlim(0, max_val)\n",
        "        plt.ylim(0, max_val)\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "\n",
        "    # Report metrics: average error, RMSLE, and hit rate\n",
        "    def report(self):\n",
        "        average_error = sum(self.errors) / self.size\n",
        "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
        "        hits = sum(1 for color in self.colors if color == \"green\")\n",
        "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
        "        self.chart(title)\n",
        "\n",
        "    # Run the full evaluation loop\n",
        "    def run(self):\n",
        "        self.error = 0\n",
        "        for i in range(self.size):\n",
        "            self.run_datapoint(i)\n",
        "        self.report()\n",
        "\n",
        "    # Convenience method to run a test directly\n",
        "    @classmethod\n",
        "    def test(cls, function, data):\n",
        "        cls(function, data).run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c830ed3e-24ee-4af6-a07b-a1bfdcd39278",
      "metadata": {
        "id": "c830ed3e-24ee-4af6-a07b-a1bfdcd39278"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "066fef03-8338-4526-9df3-89b649ad4f0a",
      "metadata": {
        "id": "066fef03-8338-4526-9df3-89b649ad4f0a"
      },
      "source": [
        "## First, the GPT-4o-mini\n",
        "\n",
        "It's called mini, but it packs a punch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ea68e8-ab1b-4f0d-aba4-a59574d8f85e",
      "metadata": {
        "id": "66ea68e8-ab1b-4f0d-aba4-a59574d8f85e"
      },
      "outputs": [],
      "source": [
        "# First let's work on a good prompt for a Frontier model\n",
        "# Notice that I'm removing the \" to the nearest dollar\"\n",
        "# When we train our own models, we'll need to make the problem as easy as possible,\n",
        "# but a Frontier model needs no such simplification.\n",
        "\n",
        "\n",
        "def messages_for(item):\n",
        "    # Instruction for the model\n",
        "    system_message = \"You estimate prices of items. Reply only with the price, no explanation.\"\n",
        "\n",
        "    # Use the 'text' field from the dictionary and clean it up\n",
        "    user_prompt = item[\"text\"].replace(\" to the nearest dollar\", \"\").replace(\"\\n\\nPrice is $\", \"\")\n",
        "\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "        {\"role\": \"assistant\", \"content\": \"Price is $\"}\n",
        "    ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff92d61-0d27-4b0d-8b32-c9891016509b",
      "metadata": {
        "id": "4ff92d61-0d27-4b0d-8b32-c9891016509b"
      },
      "outputs": [],
      "source": [
        "# üß™ Try generating a message prompt for the first test example\n",
        "messages_for(test[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1af1888-f94a-4106-b0d8-8a70939eec4e",
      "metadata": {
        "id": "b1af1888-f94a-4106-b0d8-8a70939eec4e"
      },
      "outputs": [],
      "source": [
        "# A utility function to extract the price from a string\n",
        "\n",
        "def get_price(s):\n",
        "    s = s.replace('$','').replace(',','')\n",
        "    match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n",
        "    return float(match.group()) if match else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f138c5b7-bcc1-4085-aced-68dad1bf36b4",
      "metadata": {
        "id": "f138c5b7-bcc1-4085-aced-68dad1bf36b4"
      },
      "outputs": [],
      "source": [
        "get_price(\"The price is roughly $99.99 because blah blah\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_4o_mini(text):\n",
        "    # Wrap the string back into a dict-like format for `messages_for`\n",
        "    dummy_item = {\"text\": text}\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages_for(dummy_item),\n",
        "        seed=42,\n",
        "        max_tokens=5\n",
        "    )\n",
        "    reply = response.choices[0].message.content.strip()\n",
        "    return get_price(reply)"
      ],
      "metadata": {
        "id": "dmBCZ488ZgqK"
      },
      "id": "dmBCZ488ZgqK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tester.test(gpt_4o_mini, test)"
      ],
      "metadata": {
        "id": "UkN9usFFZldD"
      },
      "id": "UkN9usFFZldD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-4o"
      ],
      "metadata": {
        "id": "9GRttG2MY1TW"
      },
      "id": "9GRttG2MY1TW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "501a2a7a-69c8-451b-bbc0-398bcb9e1612",
      "metadata": {
        "id": "501a2a7a-69c8-451b-bbc0-398bcb9e1612"
      },
      "outputs": [],
      "source": [
        "def gpt_4o_frontier(text):\n",
        "    dummy_item = {\"text\": text}\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages_for(dummy_item),\n",
        "        seed=42,\n",
        "        max_tokens=5\n",
        "    )\n",
        "    reply = response.choices[0].message.content.strip()\n",
        "    return get_price(reply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36bdd2c9-1859-4f99-a09f-3ec83b845b30",
      "metadata": {
        "id": "36bdd2c9-1859-4f99-a09f-3ec83b845b30"
      },
      "outputs": [],
      "source": [
        "Tester.test(gpt_4o_frontier, test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## claude 3-7 sonnet"
      ],
      "metadata": {
        "id": "e3XOlYPVZPz-"
      },
      "id": "e3XOlYPVZPz-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53d941cb-5b73-44ea-b893-3a0ce9997066",
      "metadata": {
        "id": "53d941cb-5b73-44ea-b893-3a0ce9997066"
      },
      "outputs": [],
      "source": [
        "def claude_3_point_7_sonnet(text):\n",
        "    # Wrap text back into a dictionary to reuse messages_for()\n",
        "    dummy_item = {\"text\": text}\n",
        "    messages = messages_for(dummy_item)\n",
        "\n",
        "    # Extract system message and user/assistant messages\n",
        "    system_message = messages[0]['content']\n",
        "    chat_messages = messages[1:]\n",
        "\n",
        "    # Call Claude 3.7 Sonnet\n",
        "    response = claude.messages.create(\n",
        "        model=\"claude-3-7-sonnet-20250219\",\n",
        "        max_tokens=5,\n",
        "        system=system_message,\n",
        "        messages=chat_messages\n",
        "    )\n",
        "\n",
        "    # Extract and return numeric prediction\n",
        "    reply = response.content[0].text.strip()\n",
        "    return get_price(reply)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11dba25d-f562-40f9-9855-40b715b7fc86",
      "metadata": {
        "id": "11dba25d-f562-40f9-9855-40b715b7fc86"
      },
      "outputs": [],
      "source": [
        "# The function for Claude 3.7 Sonnet\n",
        "# It also cost me about 15 cents to run this (pricing may vary by region)\n",
        "# You can skip this and look at my results instead\n",
        "\n",
        "Tester.test(claude_3_point_7_sonnet, test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini-1.5-flash"
      ],
      "metadata": {
        "id": "KCa9JlDPJoJm"
      },
      "id": "KCa9JlDPJoJm"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def gemini_frontier(text):\n",
        "    time.sleep(4)  # wait before each request to stay under rate limit\n",
        "    dummy_item = {\"text\": text}\n",
        "    response = gemini_via_openai_client.chat.completions.create(\n",
        "        model=\"gemini-1.5-flash\",\n",
        "        messages=messages_for(dummy_item),\n",
        "        max_tokens=5\n",
        "    )\n",
        "    try:\n",
        "        reply = response.choices[0].message.content\n",
        "        return get_price(reply.strip()) if reply else 0.0\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Gemini Error:\", e)\n",
        "        return 0.0"
      ],
      "metadata": {
        "id": "Aro-7wbeJoQq"
      },
      "id": "Aro-7wbeJoQq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tester.test(gemini_frontier, test)"
      ],
      "metadata": {
        "id": "kJnRqq7hJ4iN"
      },
      "id": "kJnRqq7hJ4iN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini-2.0-flash"
      ],
      "metadata": {
        "id": "q2eY9CowV4Vu"
      },
      "id": "q2eY9CowV4Vu"
    },
    {
      "cell_type": "code",
      "source": [
        "def gemini_2(text):\n",
        "    time.sleep(6)  # wait before each request to stay under rate limit\n",
        "    dummy_item = {\"text\": text}\n",
        "    response = gemini_via_openai_client.chat.completions.create(\n",
        "        model=\"gemini-2.0-flash-exp\",\n",
        "        messages=messages_for(dummy_item),\n",
        "        max_tokens=5\n",
        "    )\n",
        "    try:\n",
        "        reply = response.choices[0].message.content\n",
        "        return get_price(reply.strip()) if reply else 0.0\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Gemini Error:\", e)\n",
        "        return 0.0"
      ],
      "metadata": {
        "id": "ZVy9hepCV4ff"
      },
      "id": "ZVy9hepCV4ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tester.test(gemini_2, test)"
      ],
      "metadata": {
        "id": "NLh7pKUUWKOW"
      },
      "id": "NLh7pKUUWKOW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deepseek-V3"
      ],
      "metadata": {
        "id": "m3_nUI-pJocv"
      },
      "id": "m3_nUI-pJocv"
    },
    {
      "cell_type": "code",
      "source": [
        "def deepseek_frontier(text):\n",
        "    dummy_item = {\"text\": text}\n",
        "    response = deepseek_via_openai_client.chat.completions.create(\n",
        "        model=\"deepseek-chat\",\n",
        "        messages=messages_for(dummy_item),\n",
        "        seed=42,\n",
        "        max_tokens=5\n",
        "    )\n",
        "    reply = response.choices[0].message.content.strip()\n",
        "    return get_price(reply)"
      ],
      "metadata": {
        "id": "NrFA-2qKJolS"
      },
      "id": "NrFA-2qKJolS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tester.test(deepseek_frontier, test)"
      ],
      "metadata": {
        "id": "BOKjf2CdLHPs"
      },
      "id": "BOKjf2CdLHPs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}