{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§ª Baseline Model Comparison for Electronics Price Prediction\n",
        "\n",
        "In this notebook, we compare several baseline models for predicting product prices from prompt-style text inputs.  \n",
        "The goal is to establish strong non-LLM baselines before evaluating or fine-tuning large language models.\n",
        "\n",
        "### ðŸ§ª Models Included\n",
        "- ðŸŽ² **Random Price Predictor**\n",
        "- ðŸ“Š **Constant (Average Price) Predictor**\n",
        "- ðŸ“¦ **Bag of Words + Linear Regression**\n",
        "- ðŸ”  **Word2Vec + Linear Regression**\n",
        "- ðŸ“ˆ **Word2Vec + Support Vector Regressor (SVR)**\n",
        "- ðŸŒ² **Random Forest Regressor**\n",
        "\n",
        "\n",
        "Each model is evaluated using:\n",
        "- Absolute error\n",
        "- RMSLE (log-based error)\n",
        "- Hit rate (how often predictions fall within an acceptable error range)\n",
        "\n",
        "Results are visualized using scatter plots comparing model predictions vs ground truth prices from test set.\n",
        "\n",
        "> This provides a clear performance baseline before bringing in LLM-based approaches like LLaMA.\n"
      ],
      "metadata": {
        "id": "3PWWg3ypXBI1"
      },
      "id": "3PWWg3ypXBI1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ’¡ **Tip:** In Colab, selecting a GPU like the **L4** gives you access to a **powerful CPU** as well (e.g., 12 cores).\n",
        "Even if you donâ€™t need the GPU, the extra CPU power can significantly speed up our experiments.\n"
      ],
      "metadata": {
        "id": "GXMuaKUIWNwO"
      },
      "id": "GXMuaKUIWNwO"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.cpu_count()\n"
      ],
      "metadata": {
        "id": "pbW6A7rtSW0m"
      },
      "id": "pbW6A7rtSW0m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "tvGeOedSIMmd"
      },
      "id": "tvGeOedSIMmd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "681c717b-4c24-4ac3-a5f3-3c5881d6e70a",
      "metadata": {
        "id": "681c717b-4c24-4ac3-a5f3-3c5881d6e70a"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "from huggingface_hub import login\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset, Dataset, DatasetDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "933b6e75-3661-4f30-b0b5-c28d04e3748e",
      "metadata": {
        "id": "933b6e75-3661-4f30-b0b5-c28d04e3748e"
      },
      "outputs": [],
      "source": [
        "# More imports for our traditional machine learning\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42cf33b7-7abd-44ba-9780-c156b70473b5",
      "metadata": {
        "id": "42cf33b7-7abd-44ba-9780-c156b70473b5"
      },
      "outputs": [],
      "source": [
        "# NLP related imports\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1ac3ec0-183c-4a12-920b-b06397f86815",
      "metadata": {
        "id": "a1ac3ec0-183c-4a12-920b-b06397f86815"
      },
      "outputs": [],
      "source": [
        "# Finally, more imports for more advanced machine learning\n",
        "\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c01ee5f-c4fc-44fe-9d3a-907e8a0426d2",
      "metadata": {
        "id": "6c01ee5f-c4fc-44fe-9d3a-907e8a0426d2"
      },
      "outputs": [],
      "source": [
        "# Constants - used for printing to stdout in color\n",
        "\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\"\n",
        "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}\n",
        "\n",
        "HF_USER = \"vassilis19\" # your HF name here! Or use mine if you just want to reproduce my results.\n",
        "\n",
        "# Dataset\n",
        "DATASET_NAME = f\"{HF_USER}/pricer-electronics-data\"\n",
        "REVISION = \"701eba81570388cfd60924c6fe144b27491a9ec0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dd3aad2-6f99-433c-8792-e461d2f06622",
      "metadata": {
        "id": "4dd3aad2-6f99-433c-8792-e461d2f06622"
      },
      "outputs": [],
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c830ed3e-24ee-4af6-a07b-a1bfdcd39278",
      "metadata": {
        "id": "c830ed3e-24ee-4af6-a07b-a1bfdcd39278"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“¥ Load the dataset from the Hugging Face Hub (specific commit revision for reproducibility)\n"
      ],
      "metadata": {
        "id": "_GxROMocYUTv"
      },
      "id": "_GxROMocYUTv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9b05f4-c9eb-462c-8d86-de9140a2d985",
      "metadata": {
        "id": "5c9b05f4-c9eb-462c-8d86-de9140a2d985"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(DATASET_NAME, revision = REVISION)\n",
        "train = dataset['train']\n",
        "test = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a84638f7-5ff7-4f54-8751-3ef156264aee",
      "metadata": {
        "id": "a84638f7-5ff7-4f54-8751-3ef156264aee"
      },
      "outputs": [],
      "source": [
        "# Remind ourselves the testing element\n",
        "\n",
        "test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7619c85-6e9e-48a1-8efe-c6a60471b87c",
      "metadata": {
        "id": "b7619c85-6e9e-48a1-8efe-c6a60471b87c"
      },
      "outputs": [],
      "source": [
        "# Remind a training prompt price\n",
        "\n",
        "print(train[0][\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcccf130-125a-4958-bac3-f46dfcb29b3f",
      "metadata": {
        "id": "bcccf130-125a-4958-bac3-f46dfcb29b3f"
      },
      "source": [
        "## Unveiling a mighty script that we will use a lot!\n",
        "\n",
        "A rather pleasing Test Harness that will evaluate any model against 250 items from the Test set\n",
        "\n",
        "And show us the results in a visually satisfying way.\n",
        "\n",
        "You write a function of this form:\n",
        "\n",
        "```\n",
        "def my_prediction_function(item):\n",
        "    # my code here\n",
        "    return my_estimate\n",
        "```\n",
        "\n",
        "And then you call:\n",
        "\n",
        "`Tester.test(my_prediction_function, test_dataset)`\n",
        "\n",
        "To evaluate your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5793f5c-e23e-4a74-9496-1e30dd1e8935",
      "metadata": {
        "id": "b5793f5c-e23e-4a74-9496-1e30dd1e8935"
      },
      "outputs": [],
      "source": [
        "class Tester:\n",
        "    # Initialize the tester with a predictor function, dataset, optional title, and sample size\n",
        "    def __init__(self, predictor, data, title=None, size=250):\n",
        "        self.predictor = predictor\n",
        "        self.data = data\n",
        "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
        "        self.size = size  # Number of datapoints to test\n",
        "        self.guesses = []  # Model predictions\n",
        "        self.truths = []   # Ground truth prices\n",
        "        self.errors = []   # Absolute errors\n",
        "        self.sles = []     # Squared log errors\n",
        "        self.colors = []   # Color codes for visualization\n",
        "\n",
        "    # Determine color based on error severity for visualization\n",
        "    def color_for(self, error, truth):\n",
        "        if error < 40 or error / truth < 0.2:\n",
        "            return \"green\"\n",
        "        elif error < 80 or error / truth < 0.4:\n",
        "            return \"orange\"\n",
        "        else:\n",
        "            return \"red\"\n",
        "\n",
        "    # Run prediction and error calculation for a single datapoint\n",
        "    def run_datapoint(self, i):\n",
        "        datapoint = self.data[i]\n",
        "        guess = self.predictor(datapoint[\"text\"])  # Run the model\n",
        "        truth = datapoint[\"price\"]  # True price\n",
        "        error = abs(guess - truth)  # Absolute error\n",
        "        log_error = math.log(truth + 1) - math.log(guess + 1)  # Log error\n",
        "        sle = log_error ** 2  # Squared log error\n",
        "        color = self.color_for(error, truth)  # Color for this point\n",
        "        title = datapoint[\"text\"].split(\"\\n\\n\")[1][:20] + \"...\"  # Short title snippet for display\n",
        "        # Record values for reporting\n",
        "        self.guesses.append(guess)\n",
        "        self.truths.append(truth)\n",
        "        self.errors.append(error)\n",
        "        self.sles.append(sle)\n",
        "        self.colors.append(color)\n",
        "        # Print detailed result for the datapoint\n",
        "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
        "\n",
        "    # Create a scatter plot of predictions vs ground truth\n",
        "    def chart(self, title):\n",
        "        max_error = max(self.errors)\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        max_val = max(max(self.truths), max(self.guesses))\n",
        "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)  # Diagonal line\n",
        "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)  # Plot points\n",
        "        plt.xlabel('Ground Truth')\n",
        "        plt.ylabel('Model Estimate')\n",
        "        plt.xlim(0, max_val)\n",
        "        plt.ylim(0, max_val)\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "\n",
        "    # Report metrics: average error, RMSLE, and hit rate\n",
        "    def report(self):\n",
        "        average_error = sum(self.errors) / self.size\n",
        "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
        "        hits = sum(1 for color in self.colors if color == \"green\")\n",
        "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
        "        self.chart(title)\n",
        "\n",
        "    # Run the full evaluation loop\n",
        "    def run(self):\n",
        "        self.error = 0\n",
        "        for i in range(self.size):\n",
        "            self.run_datapoint(i)\n",
        "        self.report()\n",
        "\n",
        "    # Convenience method to run a test directly\n",
        "    @classmethod\n",
        "    def test(cls, function, data):\n",
        "        cls(function, data).run()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "066fef03-8338-4526-9df3-89b649ad4f0a",
      "metadata": {
        "id": "066fef03-8338-4526-9df3-89b649ad4f0a"
      },
      "source": [
        "# Now for something basic\n",
        "\n",
        "What's the very simplest model you could imagine?\n",
        "\n",
        "Let's start with a random number generator!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ea68e8-ab1b-4f0d-aba4-a59574d8f85e",
      "metadata": {
        "id": "66ea68e8-ab1b-4f0d-aba4-a59574d8f85e"
      },
      "outputs": [],
      "source": [
        "def random_pricer(item, test):\n",
        "    return random.randrange(1,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53d941cb-5b73-44ea-b893-3a0ce9997066",
      "metadata": {
        "id": "53d941cb-5b73-44ea-b893-3a0ce9997066"
      },
      "outputs": [],
      "source": [
        "# Set the random seed\n",
        "random.seed(42)\n",
        "\n",
        "# Run our TestRunner\n",
        "Tester.test(random_pricer, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97451c73-9c1b-43a8-b3b9-9c41942e48a2",
      "metadata": {
        "id": "97451c73-9c1b-43a8-b3b9-9c41942e48a2"
      },
      "outputs": [],
      "source": [
        "# That was fun!\n",
        "# We can do better - here's another rather trivial model\n",
        "\n",
        "training_prices = [item[\"price\"] for item in train]\n",
        "training_average = sum(training_prices) / len(training_prices)\n",
        "\n",
        "def constant_pricer(item):\n",
        "    return training_average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf384eb-30c2-40d8-b7e5-48942ac6a969",
      "metadata": {
        "id": "8cf384eb-30c2-40d8-b7e5-48942ac6a969"
      },
      "outputs": [],
      "source": [
        "# Run our constant predictor\n",
        "Tester.test(constant_pricer, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e1574b-52ef-49cc-bfb5-e97252ed5db8",
      "metadata": {
        "id": "79e1574b-52ef-49cc-bfb5-e97252ed5db8"
      },
      "outputs": [],
      "source": [
        "# For the next few models, we prepare our documents and prices\n",
        "# Note that we use the test prompt for the documents, otherwise we'll reveal the answer!!\n",
        "\n",
        "prices = np.array([float(item[\"price\"]) for item in train])\n",
        "documents = [item[\"text\"] for item in train]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e126c22e-53e7-4967-9ebb-6b7dd7fe4ade",
      "metadata": {
        "id": "e126c22e-53e7-4967-9ebb-6b7dd7fe4ade"
      },
      "outputs": [],
      "source": [
        "# Use the CountVectorizer for a Bag of Words model\n",
        "\n",
        "np.random.seed(42)\n",
        "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
        "X = vectorizer.fit_transform(documents)\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X, prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b7148d3-3202-4536-a75c-1627495c51d3",
      "metadata": {
        "id": "4b7148d3-3202-4536-a75c-1627495c51d3"
      },
      "outputs": [],
      "source": [
        "def bow_lr_pricer(text):\n",
        "    x = vectorizer.transform([text])  # text is a string (the prompt)\n",
        "    return max(regressor.predict(x)[0], 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f7f7d0-d22c-4282-92e5-9666a7b8535d",
      "metadata": {
        "id": "38f7f7d0-d22c-4282-92e5-9666a7b8535d"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "\n",
        "Tester.test(bow_lr_pricer, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b623079e-54fa-418f-b209-7d54ebbcc23a",
      "metadata": {
        "id": "b623079e-54fa-418f-b209-7d54ebbcc23a"
      },
      "outputs": [],
      "source": [
        "# The amazing word2vec model, implemented in gensim NLP library\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Preprocess the documents\n",
        "processed_docs = [simple_preprocess(doc) for doc in documents]\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=processed_docs, vector_size=400, window=5, min_count=1, workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de4efc7-68a6-4443-b9fd-70ee9d722362",
      "metadata": {
        "id": "3de4efc7-68a6-4443-b9fd-70ee9d722362"
      },
      "outputs": [],
      "source": [
        "# This step of averaging vectors across the document is a weakness in our approach\n",
        "\n",
        "def document_vector(doc):\n",
        "    doc_words = simple_preprocess(doc)\n",
        "    word_vectors = [w2v_model.wv[word] for word in doc_words if word in w2v_model.wv]\n",
        "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(w2v_model.vector_size)\n",
        "\n",
        "# Create feature matrix\n",
        "X_w2v = np.array([document_vector(doc) for doc in documents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f05eeec-dab8-4007-8e8c-dcf4175b8861",
      "metadata": {
        "id": "9f05eeec-dab8-4007-8e8c-dcf4175b8861"
      },
      "outputs": [],
      "source": [
        "# Run Linear Regression on word2vec\n",
        "\n",
        "word2vec_lr_regressor = LinearRegression()\n",
        "word2vec_lr_regressor.fit(X_w2v, prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43d3fb9-e013-4573-90bf-9a522132b555",
      "metadata": {
        "id": "e43d3fb9-e013-4573-90bf-9a522132b555"
      },
      "outputs": [],
      "source": [
        "def word2vec_lr_pricer(text):\n",
        "    doc_vector = document_vector(text)  # `text` is already the prompt string\n",
        "    return max(0, word2vec_lr_regressor.predict([doc_vector])[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6740319d-5c8e-4125-9106-97e2e8ab72c7",
      "metadata": {
        "id": "6740319d-5c8e-4125-9106-97e2e8ab72c7"
      },
      "outputs": [],
      "source": [
        "Tester.test(word2vec_lr_pricer, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d6d3265-37c1-464c-a489-5be4df0a7276",
      "metadata": {
        "id": "9d6d3265-37c1-464c-a489-5be4df0a7276"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machines\n",
        "\n",
        "np.random.seed(42)\n",
        "svr_regressor = LinearSVR()\n",
        "\n",
        "svr_regressor.fit(X_w2v, prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcc289e6-56a1-4119-864f-2fdf8efde643",
      "metadata": {
        "id": "fcc289e6-56a1-4119-864f-2fdf8efde643"
      },
      "outputs": [],
      "source": [
        "def svr_pricer(text):\n",
        "    np.random.seed(42)\n",
        "    doc_vector = document_vector(text)\n",
        "    return max(float(svr_regressor.predict([doc_vector])[0]), 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80286a48-7cca-40e6-af76-a814a23bb9dc",
      "metadata": {
        "id": "80286a48-7cca-40e6-af76-a814a23bb9dc"
      },
      "outputs": [],
      "source": [
        "Tester.test(svr_pricer, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c44fe4-e4d9-4559-a8ed-d8f97e25b69f",
      "metadata": {
        "id": "c6c44fe4-e4d9-4559-a8ed-d8f97e25b69f"
      },
      "outputs": [],
      "source": [
        "# And the powerful Random Forest regression\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=8)\n",
        "rf_model.fit(X_w2v, prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a38812d0-913b-400b-804f-51434d895d05",
      "metadata": {
        "id": "a38812d0-913b-400b-804f-51434d895d05"
      },
      "outputs": [],
      "source": [
        "def random_forest_pricer(text):\n",
        "    doc_vector = document_vector(text)\n",
        "    return max(0, rf_model.predict([doc_vector])[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88b51c01-c791-4fdc-8010-00b2e486b8ce",
      "metadata": {
        "id": "88b51c01-c791-4fdc-8010-00b2e486b8ce"
      },
      "outputs": [],
      "source": [
        "Tester.test(random_forest_pricer, test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}