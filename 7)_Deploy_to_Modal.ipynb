{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’°  Deploying an Electronics Price Prediction Model with Modal\n",
        "\n",
        "This notebook deploys a fine-tuned **LLaMA 3.1** model to Modal as a scalable cloud function.  \n",
        "It uses efficient 4-bit quantization (QLoRA) to serve product **price predictions** based on textual descriptions.\n",
        "\n",
        "You will:\n",
        "- Authenticate securely with Modal and Hugging Face\n",
        "- Define and deploy a `price()` function using `modal.App`\n",
        "- Load the fine-tuned model from Hugging Face\n",
        "- Query the model remotely to estimate prices for product descriptions\n",
        "\n",
        "> ðŸ’¡ Perfect for creating lightweight, serverless ML-powered services!\n"
      ],
      "metadata": {
        "id": "szq6zXOZLr8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ðŸ“¦ Install the Modal SDK quietly (for deploying and calling cloud functions)"
      ],
      "metadata": {
        "id": "FYRZfB41JUsr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdqM84X8jLgA"
      },
      "outputs": [],
      "source": [
        "!pip install -q modal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import modal # ðŸ”Œ Import the core Modal package for interacting with cloud functions\n",
        "from modal import App, Image # ðŸ› ï¸ Import specific utilities to define apps and images\n",
        "from google.colab import userdata # ðŸ” Import Colab's secure secret store for accessing user tokens safely\n",
        "import os"
      ],
      "metadata": {
        "id": "CzyXIKv16412"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ðŸ” Retrieve Modal credentials securely from Colab's secret storage\n"
      ],
      "metadata": {
        "id": "g-gGzsUPJtUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modal_token_id = userdata.get(\"MODAL_TOKEN_ID\")\n",
        "modal_token_secret = userdata.get(\"MODAL_TOKEN_SECRET\")\n",
        "\n",
        "# Set environment variables for Modal CLI to use\n",
        "os.environ[\"MODAL_TOKEN_ID\"] = modal_token_id\n",
        "os.environ[\"MODAL_TOKEN_SECRET\"] = modal_token_secret"
      ],
      "metadata": {
        "id": "4hnsGUiy-JKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ” Authenticate with Modal\n",
        "\n",
        "Before running the deployment or calling your Modal function, make sure youâ€™ve added your `MODAL_TOKEN_ID` and `MODAL_TOKEN_SECRET` as notebook secrets.\n",
        "\n",
        "> ðŸ’¡ You can do this by clicking the ðŸ”‘ key icon on the left panel in Google Colab and adding:\n",
        "> - `MODAL_TOKEN_ID`: your Modal token ID (starts with `ak-`)\n",
        "> - `MODAL_TOKEN_SECRET`: your Modal secret key (starts with `sk-`)\n",
        "\n",
        "Then, run the following command in a code cell to log in:\n"
      ],
      "metadata": {
        "id": "hva4-EwEJkoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!modal token set --token-id {modal_token_id} --token-secret {modal_token_secret}"
      ],
      "metadata": {
        "id": "0JW5whgt7a5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“¦ Define and Save Modal Function: Price Prediction with LLaMA 3.1 fine tuned\n",
        "\n",
        "This cell creates a Python script (`pricer_electronics_modal_app.py`) that defines a **Modal app** for predicting product prices using a fine-tuned LLaMA 3.1 model.\n",
        "\n",
        "#### âœ… What it does:\n",
        "- Creates a custom Modal container and installs all required libraries.\n",
        "- Downloads and loads both the base **Meta LLaMA 3.1** model and the fine-tuned weights from Hugging Face.\n",
        "- Applies **QLoRA 4-bit quantization** for faster and more memory-efficient inference on GPU.\n",
        "- Defines a `price()` method that accepts a product description and returns the predicted price.\n",
        "\n",
        "> âš ï¸ **Before deploying**, make sure youâ€™ve added a **Hugging Face token as a secret** to your Modal account:\n",
        ">\n",
        "> - **Secret name**: `hf-secret`  \n",
        "> - **Key**: `HF_TOKEN`  \n",
        "> - **Value**: *(your Hugging Face access token)*\n",
        ">\n",
        "> You can create and manage secrets at [https://modal.com/secrets](https://modal.com/secrets).\n"
      ],
      "metadata": {
        "id": "OItVh4lyCaJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pricer_electronics_modal_app.py\n",
        "\n",
        "# ðŸ“¦ Import required Modal components\n",
        "import modal\n",
        "from modal import App, Volume, Image\n",
        "\n",
        "# ðŸ› ï¸ Define the Modal application and environment\n",
        "app = modal.App(\"pricer-electronics-service\")  # Name of your Modal app\n",
        "\n",
        "# ðŸ”§ Define a base image and install required packages into it\n",
        "image = Image.debian_slim().pip_install(\n",
        "    \"huggingface\", \"torch\", \"transformers\", \"bitsandbytes\", \"accelerate\", \"peft\"\n",
        ")\n",
        "\n",
        "# ðŸ” Load secrets from Modal dashboard (specifically the Hugging Face token)\n",
        "secrets = [modal.Secret.from_name(\"hf-secret\")]\n",
        "\n",
        "# ðŸ”¢ Define constants for model loading\n",
        "GPU = \"T4\"  # Use NVIDIA T4 GPU for inference\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "PROJECT_NAME = \"pricer-electronics\"\n",
        "HF_USER = \"vassilis19\"\n",
        "RUN_NAME = \"2025-04-13_07.20.29\"\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "REVISION = \"565999daf03888afae81cadf4c8ce8e0bde9d210\"  # Commit hash for the fine-tuned model\n",
        "FINETUNED_MODEL = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "# ðŸ’¾ Local paths where models will be cached\n",
        "MODEL_DIR = \"hf-cache/\"\n",
        "BASE_DIR = MODEL_DIR + BASE_MODEL\n",
        "FINETUNED_DIR = MODEL_DIR + FINETUNED_MODEL\n",
        "\n",
        "# ðŸ“‹ Prompt formatting\n",
        "QUESTION = \"How much does this cost to the nearest dollar?\"\n",
        "PREFIX = \"Price is $\"\n",
        "\n",
        "# ðŸš€ Define the Modal class that will run the model\n",
        "@app.cls(image=image, secrets=secrets, gpu=GPU, timeout=1800)\n",
        "class Pricer:\n",
        "\n",
        "    # ðŸ› ï¸ Build-time function: Download both base and fine-tuned models to local folders\n",
        "    @modal.build()\n",
        "    def download_model_to_folder(self):\n",
        "        from huggingface_hub import snapshot_download\n",
        "        import os\n",
        "        os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "        snapshot_download(BASE_MODEL, local_dir=BASE_DIR)\n",
        "        snapshot_download(FINETUNED_MODEL, revision=REVISION, local_dir=FINETUNED_DIR)\n",
        "\n",
        "    # âœ… Runs once on container startup to load models into memory\n",
        "    @modal.enter()\n",
        "    def setup(self):\n",
        "        import os\n",
        "        import torch\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
        "        from peft import PeftModel\n",
        "\n",
        "        # âš™ï¸ Setup 4-bit quantization config\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "\n",
        "        # ðŸ”“ Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(BASE_DIR)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.tokenizer.padding_side = \"right\"\n",
        "\n",
        "        # ðŸ§  Load base model with quantization\n",
        "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            BASE_DIR,\n",
        "            quantization_config=quant_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # ðŸ” Load the fine-tuned model using PEFT\n",
        "        self.fine_tuned_model = PeftModel.from_pretrained(self.base_model, FINETUNED_DIR, revision=REVISION)\n",
        "\n",
        "    # ðŸ“ˆ Method exposed as an API endpoint to predict product price\n",
        "    @modal.method()\n",
        "    def price(self, description: str) -> float:\n",
        "        import os\n",
        "        import re\n",
        "        import torch\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
        "        from peft import PeftModel\n",
        "\n",
        "        set_seed(42)\n",
        "        prompt = f\"{QUESTION}\\n\\n{description}\\n\\n{PREFIX}\"  # Construct prompt\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "        attention_mask = torch.ones(inputs.shape, device=\"cuda\")\n",
        "\n",
        "        # ðŸ§  Run inference\n",
        "        outputs = self.fine_tuned_model.generate(\n",
        "            inputs,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=5,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "\n",
        "        result = self.tokenizer.decode(outputs[0])\n",
        "\n",
        "        # ðŸ’² Extract numeric price from the model output\n",
        "        contents = result.split(\"Price is $\")[1]\n",
        "        contents = contents.replace(',', '')\n",
        "        match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", contents)\n",
        "        return float(match.group()) if match else 0\n",
        "\n",
        "    # âš™ï¸ Ping endpoint to keep the container warm\n",
        "    @modal.method()\n",
        "    def wake_up(self) -> str:\n",
        "        return \"ok\"\n"
      ],
      "metadata": {
        "id": "xt2df-6XY878"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ðŸš€ Deploy the Modal app to the cloud (this will build the image and upload the function)\n"
      ],
      "metadata": {
        "id": "ZJ5WYc4xLM-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!modal deploy -m pricer_electronics_modal_app"
      ],
      "metadata": {
        "id": "fmTfShVDDnza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  ðŸš€ Load Deployed Modal Class\n"
      ],
      "metadata": {
        "id": "st1pENoYLU1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the deployed class\n",
        "pricer_cls = modal.Cls.from_name(\"pricer-electronics-service\", \"Pricer\")\n",
        "# Instantiate it\n",
        "pricer = pricer_cls()"
      ],
      "metadata": {
        "id": "wY3AaRcSDyx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  ðŸš€ Call the deployed Modal function with a sample product description\n"
      ],
      "metadata": {
        "id": "T-sO6YQzLbbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Call the `price` method remotely\n",
        "result = pricer.price.remote(\"Xiaomi Redmi Note 9S Dual SIM (4GB/64GB) Glacier White.\")\n",
        "\n",
        "print(f\"Predicted Price: ${result:.2f}\")"
      ],
      "metadata": {
        "id": "ncZ5RGDQGUoy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}